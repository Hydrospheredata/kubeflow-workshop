{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl \n",
    "import kfp.compiler as compiler\n",
    "import kubernetes.client.models as k8s\n",
    "import namesgenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we will obtain all training data for our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create working file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > 01_download.py << EOL\n",
    "from PIL import Image\n",
    "import struct, numpy\n",
    "import os, gzip, tarfile, shutil, glob\n",
    "import urllib, urllib.parse, urllib.request\n",
    "\n",
    "filenames = [\n",
    "    'train-images-idx3-ubyte.gz',\n",
    "    'train-labels-idx1-ubyte.gz',\n",
    "    't10k-images-idx3-ubyte.gz',\n",
    "    't10k-labels-idx1-ubyte.gz'\n",
    "]\n",
    "\n",
    "def download_files(base_url, base_dir, filenames=None):\n",
    "    \"\"\" Download required data \"\"\"\n",
    "    if not filenames: \n",
    "        # if not any filenames provided, use global instead\n",
    "        filenames = globals()[\"filenames\"]\n",
    "    \n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for file in filenames:\n",
    "        print(\"Started downloading {}\".format(file), flush=True)\n",
    "        download_url = urllib.parse.urljoin(base_url, file)\n",
    "        download_path = os.path.join(base_dir, file)\n",
    "        local_file, _ = urllib.request.urlretrieve(download_url, download_path)\n",
    "        unpack_archive(local_file, base_dir)\n",
    "\n",
    "def unpack_archive(file, base_dir):\n",
    "    \"\"\" Unpack compressed file \"\"\"\n",
    "\n",
    "    print(\"Unpacking archive {}\".format(file), flush=True)\n",
    "    with gzip.open(file, 'rb') as f_in, open(file[:-3],'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    os.remove(file)\n",
    "\n",
    "def process_images(path, dataset):\n",
    "    \"\"\" Preprocess downloaded MNIST datasets \"\"\"\n",
    "    \n",
    "    print(\"Processing images {}\".format(os.path.join(path, dataset)), flush=True)\n",
    "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
    "    with open(label_file, 'rb') as file:\n",
    "        _, num = struct.unpack(\">II\", file.read(8))\n",
    "        labels = numpy.fromfile(file, dtype=numpy.int8) #int8\n",
    "        new_labels = numpy.zeros((num, 10))\n",
    "        new_labels[numpy.arange(num), labels] = 1\n",
    "\n",
    "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
    "    with open(img_file, 'rb') as file:\n",
    "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        imgs = numpy.fromfile(file, dtype=numpy.uint8).reshape(num, rows, cols) #uint8\n",
    "        imgs = imgs.astype(numpy.float32) / 255.0\n",
    "\n",
    "    os.remove(label_file); os.remove(img_file)\n",
    "    print(\"Saving files under {} path\".format(os.path.join(path, dataset)), flush=True)\n",
    "    numpy.savez_compressed(os.path.join(path, dataset), imgs=imgs, labels=labels)\n",
    "\n",
    "def download_mnist(base_url, base_dir):\n",
    "    \"\"\" Download original MNIST structs and pack them into numpy arrays \"\"\"\n",
    "\n",
    "    download_files(base_url, base_dir)\n",
    "    process_images(base_dir, \"train\")\n",
    "    process_images(base_dir, \"t10k\") \n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    mount_path = os.environ.get(\"MOUNT_PATH\", \"./\")\n",
    "    data_path = os.path.join(mount_path, \"data\", \"mnist\")\n",
    "    download_mnist(\n",
    "        base_url=\"http://yann.lecun.com/exdb/mnist/\",\n",
    "        base_dir=data_path)\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cat > 01_Dockerfile << EOL\n",
    "FROM tidylobster/odsc-base:1.0\n",
    "ADD ./01_download.py /src/download.py\n",
    "WORKDIR /src/\n",
    "ENTRYPOINT [ \"python\", \"download.py\" ]\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build & publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  140.3MB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./01_download.py /src/download.py\n",
      " ---> 10d83625539c\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in f0886124a87f\n",
      "Removing intermediate container f0886124a87f\n",
      " ---> 845ced88d068\n",
      "Step 4/4 : ENTRYPOINT [ \"python\", \"download.py\" ]\n",
      " ---> Running in 2c979ed6faca\n",
      "Removing intermediate container 2c979ed6faca\n",
      " ---> a520dd169843\n",
      "Successfully built a520dd169843\n",
      "Successfully tagged tidylobster/mnist-pipeline-download:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-download]\n",
      "201688e90ce7: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "b2fd8b4c3da7: Waiting\n",
      "93ed1238773c: Waiting\n",
      "0fc100fdc7f9: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "eeb5ce6b3db4: Waiting\n",
      "886601877ba4: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "83c720aa6f39: Layer already exists\n",
      "93ed1238773c: Layer already exists\n",
      "eeb5ce6b3db4: Layer already exists\n",
      "56995c671038: Layer already exists\n",
      "eee35c27cf87: Layer already exists\n",
      "66a75017de07: Layer already exists\n",
      "886601877ba4: Layer already exists\n",
      "68dda0c9a8cd: Layer already exists\n",
      "f67191ae09b8: Layer already exists\n",
      "0fc100fdc7f9: Layer already exists\n",
      "201688e90ce7: Pushed\n",
      "b2fd8b4c3da7: Layer already exists\n",
      "0de2edf7bff4: Layer already exists\n",
      "latest: digest: sha256:cbe4ecf512d9b194de03a6e039ded54840d4f4aecc64f3537af57a951b78f767 size: 3041\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-download:latest -f 01_Dockerfile --no-cache . \n",
    "docker push tidylobster/mnist-pipeline-download:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Kubernetes PVC resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_pvc = k8s.V1PersistentVolumeClaimVolumeSource(claim_name=\"storage\")\n",
    "storage_volume = k8s.V1Volume(name=\"storage\", persistent_volume_claim=storage_pvc)\n",
    "storage_volume_mount = k8s.V1VolumeMount(\n",
    "    mount_path=\"{{workflow.parameters.mount-path}}\", name=\"storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required environmnet variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_path_env = k8s.V1EnvVar(name=\"MOUNT_PATH\", value=\"{{workflow.parameters.mount-path}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_op():\n",
    "    download = dsl.ContainerOp(name=\"download\",\n",
    "        image=\"tidylobster/mnist-pipeline-download:latest\")  # <-- Replace with built docker image\n",
    "    download.add_volume(storage_volume)\n",
    "    download.add_volume_mount(storage_volume_mount)\n",
    "    download.add_env_variable(mount_path_env)\n",
    "    return download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "):\n",
    "    download = download_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipelines client\n",
    "client = kfp.Client(\"http://d0958ac4.kubeflow.odsc.k8s.hydrosphere.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an experiment name\n",
    "experiment_name='MNIST Showreal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dazzling_nobel'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a name for the run\n",
    "run_name = namesgenerator.get_random_name()\n",
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create an experiment_id\n",
    "try:\n",
    "    experiment_id = client.get_experiment(experiment_name=experiment_name).id\n",
    "except:\n",
    "    experiment_id = client.create_experiment(experiment_name).id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://d0958ac4.kubeflow.odsc.k8s.hydrosphere.io/#/runs/details/06f1303f-651f-11e9-a794-02ee3ec3938a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a run\n",
    "result = client.run_pipeline(experiment_id, run_name, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we will create a model & train it on the downloaded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create working file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > 02_train.py << EOL\n",
    "import os, json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "mount_path = os.environ.get(\"MOUNT_PATH\", \"./\")\n",
    "models_path = os.path.join(mount_path, \"models\")\n",
    "data_path = os.path.join(mount_path, \"data\", \"mnist\")\n",
    "dev_env = int(os.environ.get(\"DEV_ENV\", \"0\"))\n",
    "recurring_run = int(os.environ.get(\"RECURRING_RUN\", \"0\"))\n",
    "\n",
    "if recurring_run:\n",
    "    train_file = \"subsample-train.npz\"\n",
    "    test_file = \"subsample-test.npz\"\n",
    "else: \n",
    "    train_file = \"train.npz\"\n",
    "    test_file = \"t10k.npz\"\n",
    "\n",
    "learning_rate = float(os.environ.get(\"LEARNING_RATE\", 0.01))\n",
    "epochs = int(os.environ.get(\"EPOCHS\", 10))\n",
    "batch_size = int(os.environ.get(\"BATCH_SIZE\", 256))\n",
    "\n",
    "\n",
    "def input_fn(file, shuffle=True):\n",
    "    with np.load(os.path.join(data_path, file)) as data:\n",
    "        imgs = data[\"imgs\"]\n",
    "        labels = data[\"labels\"].astype(int)\n",
    "    return tf.estimator.inputs.numpy_input_fn(x={\"imgs\": imgs}, y=labels, \n",
    "        shuffle=shuffle, batch_size=batch_size, num_epochs=epochs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    # Prepare data inputs\n",
    "    img_feature_column = tf.feature_column.numeric_column(\"imgs\", shape=(28,28))\n",
    "    train_fn, test_fn = input_fn(train_file), input_fn(test_file)\n",
    "\n",
    "    # Create the model\n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "        n_classes=10,\n",
    "        hidden_units=[256, 64],\n",
    "        feature_columns=[img_feature_column],\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate))\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    estimator.train(train_fn)\n",
    "    evaluation = estimator.evaluate(test_fn)\n",
    "    accuracy = float(evaluation[\"accuracy\"])\n",
    "\n",
    "    # Export the model \n",
    "    serving_input_receiver_fn = tf.estimator \\\n",
    "        .export.build_raw_serving_input_receiver_fn(\n",
    "            {\"imgs\": tf.placeholder(tf.float32, shape=(None, 28, 28))})\n",
    "    estimator.export_savedmodel(models_path, serving_input_receiver_fn)\n",
    "\n",
    "    # Perform metrics calculations\n",
    "    accuracy_file = \"./accuracy.txt\" if dev_env else \"/accuracy.txt\"\n",
    "    metrics_file = \"./mlpipeline-metrics.json\" if dev_env else \"/mlpipeline-metrics.json\"\n",
    "    \n",
    "    metrics = {\n",
    "        'metrics': [\n",
    "            {\n",
    "                'name': 'accuracy-score',   # -- The name of the metric. Visualized as the column \n",
    "                                            # name in the runs table.\n",
    "                'numberValue': accuracy,    # -- The value of the metric. Must be a numeric value.\n",
    "                'format': \"PERCENTAGE\",     # -- The optional format of the metric. Supported values are \n",
    "                                            # \"RAW\" (displayed in raw format) and \"PERCENTAGE\" \n",
    "                                            # (displayed in percentage format).\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Dump metrics\n",
    "    with open(accuracy_file, \"w+\") as file:\n",
    "        file.write(str(accuracy))\n",
    "    \n",
    "    with open(metrics_file, \"w+\") as file:\n",
    "        json.dump(metrics, file)\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cat > 02_Dockerfile << EOL\n",
    "FROM tidylobster/odsc-base:1.0\n",
    "ADD ./02_train.py /src/train.py\n",
    "WORKDIR /src/\n",
    "ENTRYPOINT [ \"python\", \"train.py\" ]\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  140.3MB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./02_train.py /src/train.py\n",
      " ---> 027c25d137aa\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in 931a8c43b3f0\n",
      "Removing intermediate container 931a8c43b3f0\n",
      " ---> 88a3890b7600\n",
      "Step 4/4 : ENTRYPOINT [ \"python\", \"train.py\" ]\n",
      " ---> Running in 7a385dd80dc3\n",
      "Removing intermediate container 7a385dd80dc3\n",
      " ---> 1aff4627778c\n",
      "Successfully built 1aff4627778c\n",
      "Successfully tagged tidylobster/mnist-pipeline-train:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-train]\n",
      "3cfa4e88bb4d: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "0fc100fdc7f9: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "93ed1238773c: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "eeb5ce6b3db4: Waiting\n",
      "886601877ba4: Waiting\n",
      "56995c671038: Layer already exists\n",
      "eee35c27cf87: Layer already exists\n",
      "83c720aa6f39: Layer already exists\n",
      "66a75017de07: Layer already exists\n",
      "93ed1238773c: Layer already exists\n",
      "886601877ba4: Layer already exists\n",
      "eeb5ce6b3db4: Layer already exists\n",
      "0fc100fdc7f9: Layer already exists\n",
      "68dda0c9a8cd: Layer already exists\n",
      "b2fd8b4c3da7: Layer already exists\n",
      "0de2edf7bff4: Layer already exists\n",
      "f67191ae09b8: Layer already exists\n",
      "3cfa4e88bb4d: Pushed\n",
      "latest: digest: sha256:c4d2e495ddcf9df8336620241a14c5050ba3fc549a0d76a14290b48e830c351b size: 3041\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-train:latest -f 02_Dockerfile --no-cache . \n",
    "docker push tidylobster/mnist-pipeline-train:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required environmnet variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_env = k8s.V1EnvVar(name=\"LEARNING_RATE\", value=\"{{workflow.parameters.learning-rate}}\")\n",
    "epochs_env = k8s.V1EnvVar(name=\"EPOCHS\", value=\"{{workflow.parameters.epochs}}\")\n",
    "batch_size_env = k8s.V1EnvVar(name=\"BATCH_SIZE\", value=\"{{workflow.parameters.batch-size}}\")\n",
    "recurring_run_env = k8s.V1EnvVar(name=\"RECURRING_RUN\", value=\"{{workflow.parameters.recurring-run}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op():\n",
    "    train = dsl.ContainerOp(\n",
    "        name=\"train\",\n",
    "        image=\"tidylobster/mnist-pipeline-train:latest\",        # <-- Replace with correct docker image\n",
    "        file_outputs={\"accuracy\": \"/accuracy.txt\"})\n",
    "\n",
    "    train.add_volume(storage_volume)\n",
    "    train.add_volume_mount(storage_volume_mount)\n",
    "    train.add_env_variable(mount_path_env)\n",
    "    train.add_env_variable(learning_rate_env)\n",
    "    train.add_env_variable(epochs_env)\n",
    "    train.add_env_variable(batch_size_env)\n",
    "    train.add_env_variable(recurring_run_env)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "    learning_rate=\"0.01\",\n",
    "    epochs=\"10\",\n",
    "    batch_size=\"256\",\n",
    "    recurring_run=\"0\",\n",
    "):\n",
    "    \n",
    "    download = download_op()\n",
    "    \n",
    "    train = train_op()\n",
    "    train.after(download)\n",
    "    train.set_memory_request('2G')\n",
    "    train.set_cpu_request('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wizardly_dubinsky'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = namesgenerator.get_random_name()\n",
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://d0958ac4.kubeflow.odsc.k8s.hydrosphere.io/#/runs/details/f647620a-6524-11e9-a794-02ee3ec3938a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a run\n",
    "result = client.run_pipeline(experiment_id, run_name, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we will upload the model to Hydrosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create working file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
