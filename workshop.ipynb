{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp, os\n",
    "import kfp.dsl as dsl \n",
    "import kfp.compiler as compiler\n",
    "import kubernetes.client.models as k8s\n",
    "import namesgenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMESPACE = os.environ.get(\"NAMESPACE\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubeflow_address = f\"http://{NAMESPACE}.kubeflow.odsc.k8s.hydrosphere.io\"\n",
    "hydrosphere_address = f\"http://{NAMESPACE}.serving.odsc.k8s.hydrosphere.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with existing credentials...\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!docker login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we will obtain all training data for our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build & publish an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  73.55MB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      "1.0: Pulling from tidylobster/odsc-base\n",
      "7b722c1070cd: Already exists\n",
      "5fbf74db61f1: Already exists\n",
      "ed41cb72e5c9: Already exists\n",
      "7ea47a67709e: Already exists\n",
      "b961d2edf417: Already exists\n",
      "ebdeec4d2f68: Already exists\n",
      "575aab5f7ff1: Already exists\n",
      "c5d5adaa6198: Already exists\n",
      "707316736cfa: Already exists\n",
      "3fce84e77cad: Already exists\n",
      "c40ccc181cbc: Pulling fs layer\n",
      "3d825a728840: Pulling fs layer\n",
      "c40ccc181cbc: Download complete\n",
      "c40ccc181cbc: Pull complete\n",
      "3d825a728840: Download complete\n",
      "3d825a728840: Pull complete\n",
      "Digest: sha256:8f50b0a7ac201e9ecb2e285aa3bf94af9144bc63d26700fc9f4329a6d405b98f\n",
      "Status: Downloaded newer image for tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./download.py /src/\n",
      " ---> 642b1cf36351\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in 72bf14e70a4d\n",
      "Removing intermediate container 72bf14e70a4d\n",
      " ---> 410aecc0943c\n",
      "Step 4/4 : ENTRYPOINT [ \"python\", \"download.py\" ]\n",
      " ---> Running in 32e6819380ce\n",
      "Removing intermediate container 32e6819380ce\n",
      " ---> 4247ae7f86de\n",
      "Successfully built 4247ae7f86de\n",
      "Successfully tagged tidylobster/mnist-pipeline-download:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-download]\n",
      "e3eb123d20a1: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "68dda0c9a8cd: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "93ed1238773c: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "eeb5ce6b3db4: Waiting\n",
      "886601877ba4: Waiting\n",
      "0fc100fdc7f9: Waiting\n",
      "eee35c27cf87: Layer already exists\n",
      "66a75017de07: Layer already exists\n",
      "83c720aa6f39: Layer already exists\n",
      "56995c671038: Layer already exists\n",
      "886601877ba4: Layer already exists\n",
      "eeb5ce6b3db4: Layer already exists\n",
      "0fc100fdc7f9: Layer already exists\n",
      "93ed1238773c: Layer already exists\n",
      "68dda0c9a8cd: Layer already exists\n",
      "f67191ae09b8: Layer already exists\n",
      "0de2edf7bff4: Layer already exists\n",
      "b2fd8b4c3da7: Layer already exists\n",
      "e3eb123d20a1: Pushed\n",
      "latest: digest: sha256:5cc195f666820c8daaf20fff02e0a5e3bbc384a9e7498a98afa77ccc3457500e size: 3041\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-download:latest --no-cache 01_download\n",
    "docker push tidylobster/mnist-pipeline-download:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Kubernetes PVC resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_pvc = k8s.V1PersistentVolumeClaimVolumeSource(claim_name=\"storage\")\n",
    "storage_volume = k8s.V1Volume(name=\"storage\", persistent_volume_claim=storage_pvc)\n",
    "storage_volume_mount = k8s.V1VolumeMount(mount_path=\"{{workflow.parameters.mount-path}}\", name=\"storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"tidylobster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_op(**kwargs):\n",
    "    download = dsl.ContainerOp(\n",
    "        name=\"download\",\n",
    "        image=f\"{username}/mnist-pipeline-download:latest\",  # <-- Replace with built docker image\n",
    "        file_outputs={\"data_path\": \"/data_path.txt\"},\n",
    "        arguments=[\"--mount-path\", kwargs[\"mount_path\"]])\n",
    "    \n",
    "    download.add_volume(storage_volume)\n",
    "    download.add_volume_mount(storage_volume_mount)\n",
    "    return download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "):\n",
    "    download = download_op(\n",
    "        mount_path=mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "tar -xvf pipeline.tar.gz\n",
    "sed -i '' s/minio-service.kubeflow/minio-service.${NAMESPACE}/g pipeline.yaml\n",
    "sed -i '' s/pipeline-runner/${NAMESPACE}-pipeline-runner/g pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipelines client\n",
    "client = kfp.Client(kubeflow_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an experiment name\n",
    "experiment_name='MNIST Showreal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create an experiment_id\n",
    "try:\n",
    "    experiment_id = client.get_experiment(experiment_name=experiment_name).id\n",
    "except:\n",
    "    experiment_id = client.create_experiment(experiment_name).id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new run with the name nervous_hamilton\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(500)\nReason: Internal Server Error\nHTTP response headers: HTTPHeaderDict({'Server': 'nginx/1.15.3', 'Date': 'Fri, 26 Apr 2019 12:13:27 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'trailer': 'Grpc-Trailer-Content-Type'})\nHTTP response body: {\"error\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\",\"message\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\",\"code\":13,\"details\":[{\"@type\":\"type.googleapis.com/api.Error\",\"error_message\":\"Internal Server Error\",\"error_details\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\"}]}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c1ade9138bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamesgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting a new run with the name {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pipeline.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, experiment_id, job_name, pipeline_package_path, params, pipeline_id)\u001b[0m\n\u001b[1;32m    230\u001b[0m         pipeline_spec=spec, resource_references=[reference], name=job_name)\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api/run_service_api.py\u001b[0m in \u001b[0;36mcreate_run\u001b[0;34m(self, body, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api/run_service_api.py\u001b[0m in \u001b[0;36mcreate_run_with_http_info\u001b[0;34m(self, body, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_preload_content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_request_timeout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             collection_formats=collection_formats)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    320\u001b[0m                                    \u001b[0mresponse_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                                    \u001b[0m_return_http_data_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_formats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                                    _preload_content, _request_timeout)\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             thread = self.pool.apply_async(self.__call_api, (resource_path,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mpost_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _request_timeout=_request_timeout)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    363\u001b[0m                                          \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                                          \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                                          body=body)\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             return self.rest_client.PUT(url,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    273\u001b[0m                             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                             body=body)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     def PUT(self, url, headers=None, query_params=None, post_params=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiException\u001b[0m: (500)\nReason: Internal Server Error\nHTTP response headers: HTTPHeaderDict({'Server': 'nginx/1.15.3', 'Date': 'Fri, 26 Apr 2019 12:13:27 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'trailer': 'Grpc-Trailer-Content-Type'})\nHTTP response body: {\"error\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\",\"message\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\",\"code\":13,\"details\":[{\"@type\":\"type.googleapis.com/api.Error\",\"error_message\":\"Internal Server Error\",\"error_details\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\"}]}\n"
     ]
    }
   ],
   "source": [
    "# start a run\n",
    "run_name = namesgenerator.get_random_name()\n",
    "print(\"Starting a new run with the name {}\".format(run_name))\n",
    "result = client.run_pipeline(experiment_id, run_name, \"pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we will create a model & train it on the downloaded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  54.59MB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./train.py /src/\n",
      " ---> 6d6d3ee05d63\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in 04dc3e9b4a8d\n",
      "Removing intermediate container 04dc3e9b4a8d\n",
      " ---> 3f1cf427c4d5\n",
      "Step 4/4 : ENTRYPOINT [ \"python\", \"train.py\" ]\n",
      " ---> Running in b6887fdfdfce\n",
      "Removing intermediate container b6887fdfdfce\n",
      " ---> a5a20bfbdf98\n",
      "Successfully built a5a20bfbdf98\n",
      "Successfully tagged tidylobster/mnist-pipeline-train:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-train]\n",
      "2f081c0fe8f5: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "93ed1238773c: Waiting\n",
      "eeb5ce6b3db4: Waiting\n",
      "886601877ba4: Waiting\n",
      "0fc100fdc7f9: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "56995c671038: Layer already exists\n",
      "66a75017de07: Layer already exists\n",
      "83c720aa6f39: Layer already exists\n",
      "eee35c27cf87: Layer already exists\n",
      "0fc100fdc7f9: Layer already exists\n",
      "93ed1238773c: Layer already exists\n",
      "886601877ba4: Layer already exists\n",
      "eeb5ce6b3db4: Layer already exists\n",
      "0de2edf7bff4: Layer already exists\n",
      "f67191ae09b8: Layer already exists\n",
      "68dda0c9a8cd: Layer already exists\n",
      "b2fd8b4c3da7: Layer already exists\n",
      "2f081c0fe8f5: Pushed\n",
      "latest: digest: sha256:c44c38d1037062e5c5560dd5e33124e13ddcaff4c3120f1d3669a6e5348d3f2f size: 3041\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-train:latest --no-cache 02_train/ \n",
    "docker push tidylobster/mnist-pipeline-train:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op(download, **kwargs):\n",
    "    train = dsl.ContainerOp(\n",
    "        name=\"train\",\n",
    "        image=f\"{username}/mnist-pipeline-train:latest\",        # <-- Replace with correct docker image\n",
    "        file_outputs={\"accuracy\": \"/accuracy.txt\"},\n",
    "        arguments=[\n",
    "            \"--data-path\", download.outputs[\"data_path\"], \n",
    "            \"--mount-path\", kwargs[\"mount_path\"],\n",
    "            \"--learning-rate\", kwargs[\"learning_rate\"],\n",
    "            \"--epochs\", kwargs[\"epochs\"],\n",
    "            \"--batch-size\", kwargs[\"batch_size\"]\n",
    "        ])\n",
    "\n",
    "    train.add_volume(storage_volume)\n",
    "    train.add_volume_mount(storage_volume_mount)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "    learning_rate=\"0.01\",\n",
    "    epochs=\"10\",\n",
    "    batch_size=\"256\",\n",
    "):\n",
    "    \n",
    "    download = download_op(\n",
    "        mount_path=mount_path)\n",
    "    \n",
    "    train = train_op(\n",
    "        download, \n",
    "        mount_path=mount_path, \n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    train.after(download)\n",
    "    train.set_memory_request('1G')\n",
    "    train.set_cpu_request('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "tar -xvf pipeline.tar.gz\n",
    "sed -i '' s/minio-service.kubeflow/minio-service.${NAMESPACE}/g pipeline.yaml\n",
    "sed -i '' s/pipeline-runner/${NAMESPACE}-pipeline-runner/g pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new run with the name sad_dijkstra\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(500)\nReason: Internal Server Error\nHTTP response headers: HTTPHeaderDict({'Server': 'nginx/1.15.3', 'Date': 'Fri, 26 Apr 2019 12:10:42 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'trailer': 'Grpc-Trailer-Content-Type'})\nHTTP response body: {\"error\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\",\"message\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\",\"code\":13,\"details\":[{\"@type\":\"type.googleapis.com/api.Error\",\"error_message\":\"Internal Server Error\",\"error_details\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\"}]}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-70da84740f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m\"learning-rate\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"0.01\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"batch-size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"256\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, experiment_id, job_name, pipeline_package_path, params, pipeline_id)\u001b[0m\n\u001b[1;32m    230\u001b[0m         pipeline_spec=spec, resource_references=[reference], name=job_name)\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api/run_service_api.py\u001b[0m in \u001b[0;36mcreate_run\u001b[0;34m(self, body, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api/run_service_api.py\u001b[0m in \u001b[0;36mcreate_run_with_http_info\u001b[0;34m(self, body, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_preload_content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_request_timeout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             collection_formats=collection_formats)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    320\u001b[0m                                    \u001b[0mresponse_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                                    \u001b[0m_return_http_data_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_formats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                                    _preload_content, _request_timeout)\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             thread = self.pool.apply_async(self.__call_api, (resource_path,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mpost_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _request_timeout=_request_timeout)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    363\u001b[0m                                          \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                                          \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                                          body=body)\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             return self.rest_client.PUT(url,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    273\u001b[0m                             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                             body=body)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     def PUT(self, url, headers=None, query_params=None, post_params=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/kfp_run/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiException\u001b[0m: (500)\nReason: Internal Server Error\nHTTP response headers: HTTPHeaderDict({'Server': 'nginx/1.15.3', 'Date': 'Fri, 26 Apr 2019 12:10:42 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'trailer': 'Grpc-Trailer-Content-Type'})\nHTTP response body: {\"error\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\",\"message\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\",\"code\":13,\"details\":[{\"@type\":\"type.googleapis.com/api.Error\",\"error_message\":\"Internal Server Error\",\"error_details\":\"Failed to create a new run.: InternalServerError: Failed to create a workflow for (): Unauthorized\"}]}\n"
     ]
    }
   ],
   "source": [
    "# start a run\n",
    "run_name = namesgenerator.get_random_name()\n",
    "print(\"Starting a new run with the name {}\".format(run_name))\n",
    "result = client.run_pipeline(\n",
    "    experiment_id, run_name, \"pipeline.yaml\",\n",
    "    {\n",
    "        \"learning-rate\": \"0.01\",\n",
    "        \"batch-size\": \"256\",\n",
    "        \"epochs\": \"1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we will release the trained model to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.608kB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./execute.sh /src/\n",
      " ---> 6a00f6afe7d0\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in 3efab383681b\n",
      "Removing intermediate container 3efab383681b\n",
      " ---> 5f33f8fa38cd\n",
      "Step 4/4 : ENTRYPOINT [ \"bash\", \"execute.sh\" ]\n",
      " ---> Running in dc8fcaad7f8d\n",
      "Removing intermediate container dc8fcaad7f8d\n",
      " ---> e40a5e8e1abb\n",
      "Successfully built e40a5e8e1abb\n",
      "Successfully tagged tidylobster/mnist-pipeline-release:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-release]\n",
      "2e85e0f8fb5c: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "eeb5ce6b3db4: Waiting\n",
      "0fc100fdc7f9: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "886601877ba4: Waiting\n",
      "93ed1238773c: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "56995c671038: Mounted from tidylobster/mnist-pipeline-sample\n",
      "66a75017de07: Mounted from tidylobster/mnist-pipeline-sample\n",
      "eee35c27cf87: Mounted from tidylobster/mnist-pipeline-sample\n",
      "83c720aa6f39: Mounted from tidylobster/mnist-pipeline-sample\n",
      "2e85e0f8fb5c: Pushed\n",
      "0fc100fdc7f9: Mounted from tidylobster/mnist-pipeline-sample\n",
      "93ed1238773c: Mounted from tidylobster/mnist-pipeline-sample\n",
      "886601877ba4: Mounted from tidylobster/mnist-pipeline-sample\n",
      "eeb5ce6b3db4: Mounted from tidylobster/mnist-pipeline-sample\n",
      "68dda0c9a8cd: Mounted from tidylobster/mnist-pipeline-sample\n",
      "f67191ae09b8: Mounted from tidylobster/mnist-pipeline-sample\n",
      "b2fd8b4c3da7: Mounted from tidylobster/mnist-pipeline-sample\n",
      "0de2edf7bff4: Mounted from tidylobster/mnist-pipeline-sample\n",
      "latest: digest: sha256:16ca4737e32c405ed7089e873bb7dc4869961fbab0ce8b920d1c1821cc270709 size: 3040\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-release:latest --no-cache 03_release/ \n",
    "docker push tidylobster/mnist-pipeline-release:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_op(download, train, **kwargs):\n",
    "    release = dsl.ContainerOp(\n",
    "        name=\"release\",\n",
    "        image=f\"{username}/mnist-pipeline-release:latest\",        # <-- Replace with correct docker image\n",
    "        file_outputs={\"model-version\": \"/model-version.txt\"},\n",
    "        arguments=[\n",
    "            \"--data-path\", download.outputs[\"data_path\"],\n",
    "            \"--mount-path\", kwargs[\"mount_path\"],\n",
    "            \"--accuracy\", train.outputs[\"accuracy\"], \n",
    "            \"--hydrosphere-address\", kwargs[\"hydrosphere_address\"],\n",
    "            \"--learning-rate\", kwargs[\"learning_rate\"],\n",
    "            \"--epochs\", kwargs[\"epochs\"],\n",
    "            \"--batch-size\", kwargs[\"batch_size\"],\n",
    "        ])\n",
    "\n",
    "    release.add_volume(storage_volume) \n",
    "    release.add_volume_mount(storage_volume_mount)\n",
    "    return release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "    learning_rate=\"0.01\",\n",
    "    epochs=\"10\",\n",
    "    batch_size=\"256\",\n",
    "    model_name=\"mnist\",\n",
    "    hydrosphere_address=\"\"\n",
    "):\n",
    "    \n",
    "    download = download_op(\n",
    "        mount_path=mount_path)\n",
    "    \n",
    "    train = train_op(\n",
    "        download, \n",
    "        mount_path=mount_path, \n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    train.after(download)\n",
    "    train.set_memory_request('1G')\n",
    "    train.set_cpu_request('1')\n",
    "    \n",
    "    release = release_op(\n",
    "        download, \n",
    "        train\n",
    "        mount_path=mount_path, \n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    release.after(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "tar -xvf pipeline.tar.gz\n",
    "sed -i '' s/minio-service.kubeflow/minio-service.${NAMESPACE}/g pipeline.yaml\n",
    "sed -i '' s/pipeline-runner/${NAMESPACE}-pipeline-runner/g pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new run with the name elastic_fermi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://bbe120b4.kubeflow.odsc.k8s.hydrosphere.io/#/runs/details/9264278a-65e7-11e9-a794-02ee3ec3938a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start a run\n",
    "run_name = namesgenerator.get_random_name()\n",
    "print(\"Starting a new run with the name {}\".format(run_name))\n",
    "result = client.run_pipeline(\n",
    "    experiment_id, run_name, \"pipeline.yaml\",\n",
    "    {\n",
    "        \"learning-rate\": \"0.01\",\n",
    "        \"batch-size\": \"256\",\n",
    "        \"epochs\": \"10\",\n",
    "        \"hydrosphere-address\": hydrosphere_address,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we are deploying the model to the stage application to run integration tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  3.584kB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./execute.sh /src/\n",
      " ---> 598ffa5d76b5\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in aad2fdaf6e71\n",
      "Removing intermediate container aad2fdaf6e71\n",
      " ---> 6ff09d10870d\n",
      "Step 4/4 : ENTRYPOINT [ \"bash\", \"execute.sh\" ]\n",
      " ---> Running in 192af4c52c68\n",
      "Removing intermediate container 192af4c52c68\n",
      " ---> a3d9daef6a25\n",
      "Successfully built a3d9daef6a25\n",
      "Successfully tagged tidylobster/mnist-pipeline-deploy-to-stage:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-deploy-to-stage]\n",
      "ee538f44b6d8: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "0fc100fdc7f9: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "eeb5ce6b3db4: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "886601877ba4: Waiting\n",
      "93ed1238773c: Waiting\n",
      "eee35c27cf87: Mounted from tidylobster/mnist-pipeline-release\n",
      "83c720aa6f39: Mounted from tidylobster/mnist-pipeline-release\n",
      "56995c671038: Mounted from tidylobster/mnist-pipeline-release\n",
      "66a75017de07: Mounted from tidylobster/mnist-pipeline-release\n",
      "eeb5ce6b3db4: Mounted from tidylobster/mnist-pipeline-release\n",
      "886601877ba4: Mounted from tidylobster/mnist-pipeline-release\n",
      "ee538f44b6d8: Pushed\n",
      "93ed1238773c: Mounted from tidylobster/mnist-pipeline-release\n",
      "0fc100fdc7f9: Mounted from tidylobster/mnist-pipeline-release\n",
      "f67191ae09b8: Mounted from tidylobster/mnist-pipeline-release\n",
      "0de2edf7bff4: Mounted from tidylobster/mnist-pipeline-release\n",
      "b2fd8b4c3da7: Mounted from tidylobster/mnist-pipeline-release\n",
      "68dda0c9a8cd: Mounted from tidylobster/mnist-pipeline-release\n",
      "latest: digest: sha256:4fd7c8d4ffdf32b9e8567309b97e7799d59dbc530155fe6284aded69f497d8d6 size: 3040\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-deploy-to-stage:latest --no-cache 04_deploy-to-stage/ \n",
    "docker push tidylobster/mnist-pipeline-deploy-to-stage:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required environmnet variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_name_env = k8s.V1EnvVar(name=\"APPLICATION_NAME\", value=\"{{workflow.parameters.application-name}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_to_stage_op(release, **kwargs):\n",
    "    deploy_to_stage = dsl.ContainerOp(\n",
    "        name=\"deploy_to_stage\",\n",
    "        image=f\"{username}/mnist-pipeline-deploy-to-stage:latest\",        # <-- Replace with correct docker image\n",
    "        file_outputs={\"stage-app-name\": \"/stage-app-name.txt\"},\n",
    "        arguments=[\n",
    "            \"--model-version\", release.outputs[\"model-version\"],\n",
    "            \"--hydrosphere-address\", kwargs[\"hydrosphere_address\"],\n",
    "            \"--model-name\", kwargs[\"model_name\"],\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return deploy_to_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "    learning_rate=\"0.01\",\n",
    "    epochs=\"10\",\n",
    "    batch_size=\"256\",\n",
    "    model_name=\"mnist\",\n",
    "    hydrosphere_address=\"\",\n",
    "):\n",
    "    \n",
    "    download = download_op(\n",
    "        mount_path=mount_path)\n",
    "    \n",
    "    train = train_op(\n",
    "        download, \n",
    "        mount_path=mount_path, \n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    train.after(download)\n",
    "    train.set_memory_request('1G')\n",
    "    train.set_cpu_request('1')\n",
    "    \n",
    "    release = release_op(\n",
    "        download, \n",
    "        train\n",
    "        mount_path=mount_path, \n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    release.after(train)\n",
    "    \n",
    "    deploy_to_stage = deploy_to_stage_op(\n",
    "        release\n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        model_name=model_name)\n",
    "    deploy_to_stage.after(release)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "tar -xvf pipeline.tar.gz\n",
    "sed -i '' s/minio-service.kubeflow/minio-service.${NAMESPACE}/g pipeline.yaml\n",
    "sed -i '' s/pipeline-runner/${NAMESPACE}-pipeline-runner/g pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new run with the name optimistic_leavitt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://bbe120b4.kubeflow.odsc.k8s.hydrosphere.io/#/runs/details/a173aa63-65e7-11e9-a794-02ee3ec3938a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start a run\n",
    "run_name = namesgenerator.get_random_name()\n",
    "print(\"Starting a new run with the name {}\".format(run_name))\n",
    "result = client.run_pipeline(\n",
    "    experiment_id, run_name, \"pipeline.yaml\",\n",
    "    {\n",
    "        \"learning-rate\": \"0.01\",\n",
    "        \"batch-size\": \"256\",\n",
    "        \"epochs\": \"10\",\n",
    "        \"hydrosphere-address\": hydrosphere_address,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we are performing integration tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  32.07MB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./test.py /src/\n",
      " ---> 045e1ae7d4d4\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in 1f9ca431d3d6\n",
      "Removing intermediate container 1f9ca431d3d6\n",
      " ---> ac29e11563c5\n",
      "Step 4/4 : ENTRYPOINT [ \"python\", \"test.py\" ]\n",
      " ---> Running in dddc68c4da11\n",
      "Removing intermediate container dddc68c4da11\n",
      " ---> dda556327cc5\n",
      "Successfully built dda556327cc5\n",
      "Successfully tagged tidylobster/mnist-pipeline-test:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-test]\n",
      "61de64499d97: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "886601877ba4: Waiting\n",
      "0fc100fdc7f9: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "eeb5ce6b3db4: Waiting\n",
      "93ed1238773c: Waiting\n",
      "eee35c27cf87: Layer already exists\n",
      "83c720aa6f39: Layer already exists\n",
      "56995c671038: Layer already exists\n",
      "66a75017de07: Layer already exists\n",
      "0fc100fdc7f9: Layer already exists\n",
      "886601877ba4: Layer already exists\n",
      "eeb5ce6b3db4: Layer already exists\n",
      "93ed1238773c: Layer already exists\n",
      "68dda0c9a8cd: Layer already exists\n",
      "f67191ae09b8: Layer already exists\n",
      "b2fd8b4c3da7: Layer already exists\n",
      "0de2edf7bff4: Layer already exists\n",
      "61de64499d97: Pushed\n",
      "latest: digest: sha256:04905daf3f405d3de9d15c4f9bf2f28791385bf095a1210fd398532739d3e42d size: 3040\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-test:latest --no-cache 05_test/ \n",
    "docker push tidylobster/mnist-pipeline-test:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required environmnet variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amount_env = k8s.V1EnvVar(name=\"TEST_AMOUNT\", value=\"{{workflow.parameters.test-amount}}\")\n",
    "requests_delay_env = k8s.V1EnvVar(name=\"REQUESTS_DELAY\", value=\"{{workflow.parameters.requests-delay}}\")\n",
    "acceptable_accuracy_env = k8s.V1EnvVar(name=\"ACCEPTABLE_ACCURACY\", value=\"{{workflow.parameters.acceptable-accuracy}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_op(download, stage, **kwargs):\n",
    "    test = dsl.ContainerOp(\n",
    "        name=\"test\",\n",
    "        image=f\"{username}/mnist-pipeline-test:latest\",               # <-- Replace with correct docker image\n",
    "        arguments=[\n",
    "            \"--stage-app-name\", stage.outputs[\"stage-app-name\"], \n",
    "            \"--data-path\", download.outputs[\"data_path\"],\n",
    "            \"--mount-path\", kwargs[\"mount_path\"],\n",
    "            \"--hydrosphere-address\", kwargs[\"hydrosphere_address\"],\n",
    "            \"--acceptable-acuracy\", kwargs[\"acceptable_accuracy\"],\n",
    "            \"--model-name\", kwargs[\"model_name\"], \n",
    "        ],\n",
    "    )\n",
    "\n",
    "    test.add_volume(storage_volume) \n",
    "    test.add_volume_mount(storage_volume_mount)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "    learning_rate=\"0.01\",\n",
    "    epochs=\"10\",\n",
    "    batch_size=\"256\",\n",
    "    model_name=\"mnist\",\n",
    "    hydrosphere_address=\"\",\n",
    "    acceptable_accuracy=\"0.90\",\n",
    "):\n",
    "    \n",
    "    download = download_op(\n",
    "        mount_path=mount_path)\n",
    "    \n",
    "    train = train_op(\n",
    "        download, \n",
    "        mount_path=mount_path, \n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    train.after(download)\n",
    "    train.set_memory_request('1G')\n",
    "    train.set_cpu_request('1')\n",
    "    \n",
    "    release = release_op(\n",
    "        download, \n",
    "        train\n",
    "        mount_path=mount_path, \n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    release.after(train)\n",
    "    \n",
    "    deploy_to_stage = deploy_to_stage_op(\n",
    "        release\n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        model_name=model_name)\n",
    "    deploy_to_stage.after(release)\n",
    "    \n",
    "    test = test_op(\n",
    "        download, \n",
    "        deploy_to_stage,\n",
    "        mount_path=mount_path,\n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        acceptable_accuracy=acceptable_accuracy,\n",
    "        model_name=model_name)\n",
    "    test.set_retry(3)\n",
    "    test.after(deploy_to_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "tar -xvf pipeline.tar.gz\n",
    "sed -i '' s/minio-service.kubeflow/minio-service.${NAMESPACE}/g pipeline.yaml\n",
    "sed -i '' s/pipeline-runner/${NAMESPACE}-pipeline-runner/g pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new run with the name silly_varahamihira\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://bbe120b4.kubeflow.odsc.k8s.hydrosphere.io/#/runs/details/c448efd0-65e7-11e9-a794-02ee3ec3938a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start a run\n",
    "run_name = namesgenerator.get_random_name()\n",
    "print(\"Starting a new run with the name {}\".format(run_name))\n",
    "result = client.run_pipeline(\n",
    "    experiment_id, run_name, \"pipeline.yaml\",\n",
    "    {\n",
    "        \"learning-rate\": \"0.01\",\n",
    "        \"batch-size\": \"256\",\n",
    "        \"epochs\": \"10\",\n",
    "        \"hydrosphere-address\": hydrosphere_address,\n",
    "        \"acceptable-accuracy\": \"0.90\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally deploying the model to production application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  3.072kB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./execute.sh /src/\n",
      " ---> e285ac535611\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in ddf7dac09750\n",
      "Removing intermediate container ddf7dac09750\n",
      " ---> e4b7541b68cf\n",
      "Step 4/4 : ENTRYPOINT [ \"bash\", \"execute.sh\" ]\n",
      " ---> Running in 2a479e1132e6\n",
      "Removing intermediate container 2a479e1132e6\n",
      " ---> 84119c6bef76\n",
      "Successfully built 84119c6bef76\n",
      "Successfully tagged tidylobster/mnist-pipeline-deploy-to-prod:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-deploy-to-prod]\n",
      "537310a8c9b3: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "93ed1238773c: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "eeb5ce6b3db4: Waiting\n",
      "0fc100fdc7f9: Waiting\n",
      "886601877ba4: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "56995c671038: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "66a75017de07: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "83c720aa6f39: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "eee35c27cf87: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "537310a8c9b3: Pushed\n",
      "93ed1238773c: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "eeb5ce6b3db4: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "886601877ba4: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "0fc100fdc7f9: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "68dda0c9a8cd: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "f67191ae09b8: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "0de2edf7bff4: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "b2fd8b4c3da7: Mounted from tidylobster/mnist-pipeline-deploy-to-stage\n",
      "latest: digest: sha256:6613bd90debf1ad615bb57be1052268b6228b933cbe17081d513a399c12b5175 size: 3040\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-deploy-to-prod:latest --no-cache 06_deploy-to-prod/ \n",
    "docker push tidylobster/mnist-pipeline-deploy-to-prod:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_to_prod_op(release, **kwargs):\n",
    "    deploy_to_prod = dsl.ContainerOp(\n",
    "        name=\"deploy_to_prod\",\n",
    "        image=f\"{username}/mnist-pipeline-deploy-to-prod:latest\",   # <-- Replace with correct docker image\n",
    "        arguments=[\n",
    "            \"--model-version\", release.outputs[\"model-version\"],\n",
    "            \"--model-name\", kwargs[\"model_name\"],\n",
    "            \"--hydrosphere-address\", kwargs[\"hydrosphere_address\"]\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return deploy_to_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "    learning_rate=\"0.01\",\n",
    "    epochs=\"10\",\n",
    "    batch_size=\"256\",\n",
    "    model_name=\"mnist\",\n",
    "    hydrosphere_address=\"\",\n",
    "    acceptable_accuracy=\"0.90\",\n",
    "):\n",
    "    \n",
    "    download = download_op(\n",
    "        mount_path=mount_path)\n",
    "    \n",
    "    train = train_op(\n",
    "        download, \n",
    "        mount_path=mount_path, \n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    train.after(download)\n",
    "    train.set_memory_request('1G')\n",
    "    train.set_cpu_request('1')\n",
    "    \n",
    "    release = release_op(\n",
    "        download, \n",
    "        train\n",
    "        mount_path=mount_path, \n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    release.after(train)\n",
    "    \n",
    "    deploy_to_stage = deploy_to_stage_op(\n",
    "        release\n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        model_name=model_name)\n",
    "    deploy_to_stage.after(release)\n",
    "    \n",
    "    test = test_op(\n",
    "        download, \n",
    "        deploy_to_stage,\n",
    "        mount_path=mount_path,\n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        acceptable_accuracy=acceptable_accuracy,\n",
    "        model_name=model_name)\n",
    "    test.set_retry(3)\n",
    "    test.after(deploy_to_stage)\n",
    "    \n",
    "    deploy_to_prod = deploy_to_prod_op(\n",
    "        release,\n",
    "        model_name=model_name,\n",
    "        hydrosphere_address=hydrosphere_address)\n",
    "    deploy_to_prod.after(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "tar -xvf pipeline.tar.gz\n",
    "sed -i '' s/minio-service.kubeflow/minio-service.${NAMESPACE}/g pipeline.yaml\n",
    "sed -i '' s/pipeline-runner/${NAMESPACE}-pipeline-runner/g pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new run with the name vigorous_albattani\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://cc2645d0.kubeflow.odsc.k8s.hydrosphere.io/#/runs/details/8abe8ec8-65f1-11e9-a794-02ee3ec3938a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start a run\n",
    "run_name = namesgenerator.get_random_name()\n",
    "print(\"Starting a new run with the name {}\".format(run_name))\n",
    "result = client.run_pipeline(\n",
    "    experiment_id, run_name, \"pipeline.yaml\",\n",
    "    {\n",
    "        \"learning-rate\": \"0.01\",\n",
    "        \"batch-size\": \"256\",\n",
    "        \"epochs\": \"10\",\n",
    "        \"hydrosphere-address\": hydrosphere_address,\n",
    "        \"acceptable-accuracy\": \"0.90\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a fully functional pipeline, we would like to automate running this pipeline. But we don't "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and publish image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  145.4kB\r",
      "\r\n",
      "Step 1/4 : FROM tidylobster/odsc-base:1.0\n",
      " ---> a44d37e5b862\n",
      "Step 2/4 : ADD ./sample.py /src/\n",
      " ---> bfb058e25199\n",
      "Step 3/4 : WORKDIR /src/\n",
      " ---> Running in 29e23b657ee3\n",
      "Removing intermediate container 29e23b657ee3\n",
      " ---> 2f4747efea6a\n",
      "Step 4/4 : ENTRYPOINT [ \"python\", \"sample.py\" ]\n",
      " ---> Running in bbfe33fe0d33\n",
      "Removing intermediate container bbfe33fe0d33\n",
      " ---> 3a2a04fe2c08\n",
      "Successfully built 3a2a04fe2c08\n",
      "Successfully tagged tidylobster/mnist-pipeline-sample:latest\n",
      "The push refers to repository [docker.io/tidylobster/mnist-pipeline-sample]\n",
      "5b283f22ec6c: Preparing\n",
      "66a75017de07: Preparing\n",
      "83c720aa6f39: Preparing\n",
      "56995c671038: Preparing\n",
      "eee35c27cf87: Preparing\n",
      "93ed1238773c: Preparing\n",
      "eeb5ce6b3db4: Preparing\n",
      "886601877ba4: Preparing\n",
      "0fc100fdc7f9: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "886601877ba4: Waiting\n",
      "93ed1238773c: Waiting\n",
      "eeb5ce6b3db4: Waiting\n",
      "0fc100fdc7f9: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "56995c671038: Layer already exists\n",
      "83c720aa6f39: Layer already exists\n",
      "eee35c27cf87: Layer already exists\n",
      "66a75017de07: Layer already exists\n",
      "93ed1238773c: Layer already exists\n",
      "0fc100fdc7f9: Layer already exists\n",
      "886601877ba4: Layer already exists\n",
      "eeb5ce6b3db4: Layer already exists\n",
      "f67191ae09b8: Layer already exists\n",
      "68dda0c9a8cd: Layer already exists\n",
      "b2fd8b4c3da7: Layer already exists\n",
      "0de2edf7bff4: Layer already exists\n",
      "5b283f22ec6c: Pushed\n",
      "latest: digest: sha256:094a474b7c4e505e45e8a92294fd6728a11466dd545d425769c7829b39865bc3 size: 3041\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t tidylobster/mnist-pipeline-sample:latest --no-cache 01_sample/ \n",
    "docker push tidylobster/mnist-pipeline-sample:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define container operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_op():\n",
    "    sample = dsl.ContainerOp(\n",
    "        name=\"sample\",\n",
    "        image=f\"{username}/mnist-pipeline-sample:latest\", # <-- Replace with correct docker image\n",
    "        file_outputs={\"data_path\": \"/data_path.txt\"}\n",
    "        arguments=[\n",
    "            \"--mount-path\", kwargs[\"mount_path\"], \n",
    "            \"--hydrosphere-address\", kwargs[\"hydrosphere_address\"],\n",
    "        ]\n",
    "    )  \n",
    "    \n",
    "    sample.add_volume(storage_volume)\n",
    "    sample.add_volume_mount(storage_volume_mount)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "release@dsl.pipeline(name=\"mnist\", description=\"MNIST classifier\")\n",
    "def pipeline_definition(\n",
    "    mount_path=\"/storage\",\n",
    "    learning_rate=\"0.01\",\n",
    "    epochs=\"10\",\n",
    "    batch_size=\"256\",\n",
    "    model_name=\"mnist\",\n",
    "    hydrosphere_address=\"\",\n",
    "    acceptable_accuracy=\"0.90\",\n",
    "):\n",
    "    \n",
    "    sample = sample_op()\n",
    "    \n",
    "    train = train_op(\n",
    "        sample, \n",
    "        mount_path=mount_path, \n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    train.after(sample)\n",
    "    train.set_memory_request('1G')\n",
    "    train.set_cpu_request('1')\n",
    "    \n",
    "    release = release_op(\n",
    "        sample, \n",
    "        train\n",
    "        mount_path=mount_path, \n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "    release.after(train)\n",
    "    \n",
    "    deploy_to_stage = deploy_to_stage_op(\n",
    "        release\n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        model_name=model_name)\n",
    "    deploy_to_stage.after(release)\n",
    "    \n",
    "    test = test_op(\n",
    "        download, \n",
    "        deploy_to_stage,\n",
    "        mount_path=mount_path,\n",
    "        hydrosphere_address=hydrosphere_address,\n",
    "        acceptable_accuracy=acceptable_accuracy,\n",
    "        model_name=model_name)\n",
    "    test.set_retry(3)\n",
    "    test.after(deploy_to_stage)\n",
    "    \n",
    "    deploy_to_prod = deploy_to_prod_op(\n",
    "        release,\n",
    "        model_name=model_name,\n",
    "        hydrosphere_address=hydrosphere_address)\n",
    "    deploy_to_prod.after(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_definition, \"pipeline.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "tar -xvf pipeline.tar.gz\n",
    "sed -i '' s/minio-service.kubeflow/minio-service.${NAMESPACE}/g pipeline.yaml\n",
    "sed -i '' s/pipeline-runner/${NAMESPACE}-pipeline-runner/g pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new run with the name focused_spence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://bbe120b4.kubeflow.odsc.k8s.hydrosphere.io/#/runs/details/feb59113-65e7-11e9-a794-02ee3ec3938a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start a run\n",
    "run_name = namesgenerator.get_random_name()\n",
    "print(\"Starting a new run with the name {}\".format(run_name))\n",
    "result = client.run_pipeline(\n",
    "    experiment_id, run_name, \"pipeline.yaml\",\n",
    "    {\n",
    "        \"learning-rate\": \"0.01\",\n",
    "        \"batch-size\": \"256\",\n",
    "        \"epochs\": \"10\",\n",
    "        \"hydrosphere-address\": hydrosphere_address,\n",
    "        \"acceptable-accuracy\": \"0.90\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
